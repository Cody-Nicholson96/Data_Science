# Core Hadoop

The core Hadoop project consists of a way to **store data** known as the Hadoop distributed file system (or HDFS)

It also consists of a way to **process data** with mapReduce

-

To do this we split up the data and store it in across a collection of machines known as a cluster, then we process it where it is stored in the cluster rather than retrieving it from a central server

You can add more machines to the cluster as the amount of data accumulates

The machines don't need to be anything high-end, most clusters are built using mid-range rack-mounted servers
